\section{Existing Literature}

David Krueger and Roland Memisevic, in their work on regularizing RNNs (\url{https://arxiv.org/pdf/1511.08400v7.pdf}) used the adam optimizer with gradient clipping at 200 and a learning rate of .001. We tried to do the same, hoping that we might avoid overfitting.

A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio. 2016. \url{http://arxiv.org/abs/1605.06069}

Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models. Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau. 2016. AAAI. \url{http://arxiv.org/abs/1507.04808}

\section{References}
\label{sec:ref}

\url{https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/2-seq2seq-advanced.ipynb}

\url{https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/1-seq2seq.ipynb}

\url{https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/2-seq2seq-advanced.ipynb}

\url{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py}